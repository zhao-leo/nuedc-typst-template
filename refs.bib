
@article{xu_fakeshield_2025,
	title = {{FAKESHIELD}: {EXPLAINABLE} {IMAGE} {FORGERY} {DE}- {TECTION} {AND} {LOCALIZATION} {VIA} {MULTI}-{MODAL} {LARGE} {LANGUAGE} {MODELS}},
	abstract = {The rapid development of generative AI is a double-edged sword, which not only facilitates content creation but also makes image manipulation easier and more difficult to detect. Although current image forgery detection and localization (IFDL) methods are generally effective, they tend to face two challenges: 1) black-box nature with unknown detection principle, 2) limited generalization across diverse tampering methods (e.g., Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the explainable IFDL task and design FakeShield, a multimodal framework capable of evaluating image authenticity, generating tampered region masks, and providing a judgment basis based on pixel-level and imagelevel tampering clues. Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating the Multi-Modal Tamper Description dataSet (MMTDSet) for training FakeShield’s tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided Explainable Forgery Detection Module (DTEFDM) and a Multi-modal Forgery Localization Module (MFLM) to address various types of tamper detection interpretation and achieve forgery localization guided by detailed textual descriptions. Extensive experiments demonstrate that FakeShield effectively detects and localizes various tampering techniques, offering an explainable and superior solution compared to previous IFDL methods. The code is available at https://github.com/zhipeixu/FakeShield.},
	language = {en},
	author = {Xu, Zhipei and Zhang, Xuanyu and Li, Runyi and Tang, Zecheng and Huang, Qing and Zhang, Jian},
	year = {2025},
	file = {PDF:/home/april/Zotero/storage/AYLITMTD/Xu 等 - 2025 - FAKESHIELD EXPLAINABLE IMAGE FORGERY DE- TECTION AND LOCALIZATION VIA MULTI-MODAL LARGE LANGUAGE MO.pdf:application/pdf;zh-hans-PDF:/home/april/Zotero/storage/GMI7FFZU/ai_latex_trans_arxiv_2410.02761v4_5ddc6ac5-68df-493f-a3b4-6d849b6ffc70-trans.pdf:application/pdf},
}

@inproceedings{bakirtzis_deepray_2022,
	title = {{DeepRay}: {Deep} {Learning} {Meets} {Ray}-{Tracing}},
	shorttitle = {{DeepRay}},
	url = {https://ieeexplore.ieee.org/document/9769203/?arnumber=9769203},
	doi = {10.23919/EuCAP53622.2022.9769203},
	abstract = {Efficient and accurate indoor radio propagation modeling tools are essential for the design and operation of wireless communication systems. Lately, several attempts to combine radio propagation solvers with machine learning (ML) have been made. In this paper, motivated by the recent advances in the area of computer vision, we present a new ML propagation model using convolutional encoder-decoders. Specifically, we couple a ray-tracing simulator with either a U-Net or an SDU-Net, showing that the use of atrous convolutions utilized in SDU-Net can enhance significantly the performance of an ML propagation model. The proposed data-driven framework, called DeepRay, can be trained to predict the received signal strength in a given indoor environment. More importantly, once trained over multiple input geometries, DeepRay can be employed to directly predict the signal level for unknown indoor environments. We demonstrate this approach in various indoor environments using long range (LoRa) devices operating at 868 MHz.},
	urldate = {2025-03-16},
	booktitle = {2022 16th {European} {Conference} on {Antennas} and {Propagation} ({EuCAP})},
	author = {Bakirtzis, Stefanos and Qiu, Kehai and Zhang, Jie and Wassell, Ian},
	month = mar,
	year = {2022},
	keywords = {Antenna measurements, Computational modeling, Convolution, deep learning, Geometry, LoRA, Machine learning, radio propa-gation modeling, ray-tracing, Scalability, Transforms, U-Net, Wireless communication},
	pages = {1--5},
	file = {Full Text PDF:/home/april/Zotero/storage/VLQRSZ3M/Bakirtzis 等 - 2022 - DeepRay Deep Learning Meets Ray-Tracing.pdf:application/pdf;IEEE Xplore Abstract Record:/home/april/Zotero/storage/ASRJ6ICK/9769203.html:text/html},
}

@inproceedings{liu_moc_2024,
	address = {San Francisco CA USA},
	title = {{MoC}: {A} {Morton}-{Code}-{Based} {Fine}-{Grained} {Quantization} for {Accelerating} {Point} {Cloud} {Neural} {Networks}},
	isbn = {979-8-4007-0601-1},
	shorttitle = {{MoC}},
	url = {https://dl.acm.org/doi/10.1145/3649329.3655905},
	doi = {10.1145/3649329.3655905},
	language = {en},
	urldate = {2025-03-02},
	booktitle = {Proceedings of the 61st {ACM}/{IEEE} {Design} {Automation} {Conference}},
	publisher = {ACM},
	author = {Liu, Xueyuan and Song, Zhuoran and Chen, Hao and Li, Xing and Liang, Xiaoyao},
	month = jun,
	year = {2024},
	pages = {1--6},
	file = {PDF:/home/april/Zotero/storage/CRKXIVLX/Liu 等 - 2024 - MoC A Morton-Code-Based Fine-Grained Quantization for Accelerating Point Cloud Neural Networks.pdf:application/pdf},
}

@article{wu_edge-side_2024,
	title = {Edge-{Side} {Fine}-{Grained} {Sparse} {CNN} {Accelerator} {With} {Efficient} {Dynamic} {Pruning} {Scheme}},
	volume = {71},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1549-8328, 1558-0806},
	url = {https://ieeexplore.ieee.org/document/10385165/},
	doi = {10.1109/TCSI.2023.3347417},
	abstract = {With the rapid development of the Internet of Things (IoT), it has become a common concern of academia and industry to provide real-time high performance services for edge-side applications and to bestow intelligence on massive edgeside devices. Due to the limitations of storage space, volume and power consumption of edge side devices, it is difficult for existing convolutional neural networks with large number of parameters and large amount of computation to match them. Network pruning can effectively alleviate the excessive parameters and computation issues in CNNs. However, finegrained pruning is not hardware friendly, while other structured pruning schemes will result in a much higher loss of accuracy under the same compression ratio. In this paper, an model compression strategy is given including the proposed efficient fine-grained pruning scheme, a dynamic pruning \& training method, and a weight importance judgment method. Depending on this strategy, sparse VGG16 (ResNet50) model can be obtained by training from scratch, and achieves a total of 16× compression ratio with 1/32 indexing overhead. Further, a light-weight, highperformance sparse CNN accelerator with modified systolic array is proposed. Implementing VGG16 and ResNet50 on the proposed accelerator, the experimental results show that compared with the most advanced design, the proposed accelerator can achieve 8.13 Frames Per Second (FPS) with 2.17× better power efficiency and at most 4.14× better calculation density.},
	language = {en},
	number = {3},
	urldate = {2025-03-02},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Wu, Bi and Yu, Tianyang and Chen, Ke and Liu, Weiqiang},
	month = mar,
	year = {2024},
	pages = {1285--1298},
	file = {PDF:/home/april/Zotero/storage/UGKUW9KR/Wu 等 - 2024 - Edge-Side Fine-Grained Sparse CNN Accelerator With Efficient Dynamic Pruning Scheme.pdf:application/pdf},
}

@misc{zhang_mixpe_2024,
	title = {{MixPE}: {Quantization} and {Hardware} {Co}-design for {Efficient} {LLM} {Inference}},
	shorttitle = {{MixPE}},
	url = {http://arxiv.org/abs/2411.16158},
	doi = {10.48550/arXiv.2411.16158},
	abstract = {Transformer-based large language models (LLMs) have achieved remarkable success as model sizes continue to grow, yet their deployment remains challenging due to significant computational and memory demands. Quantization has emerged as a promising solution, and state-of-the-art quantization algorithms for LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), where lower-precision weights are multiplied with higher-precision activations. Despite its benefits, current hardware accelerators such as GPUs and TPUs lack native support for efficient mpGEMM, leading to inefficient dequantization operations in the main sequential loop. To address this limitation, we introduce MixPE, a specialized mixed-precision processing element designed for efficient low-bit quantization in LLM inference. MixPE leverages two key innovations to minimize dequantization overhead and unlock the full potential of low-bit quantization. First, recognizing that scale and zero point are shared within each quantization group, we propose performing dequantization after per-group mpGEMM, significantly reducing dequantization overhead. Second, instead of relying on conventional multipliers, MixPE utilizes efficient shift\&add operations for multiplication, optimizing both computation and energy efficiency. Our experimental results demonstrate that MixPE surpasses the state-ofthe-art quantization accelerators by 2.6× speedup and 1.4× energy reduction.},
	language = {en},
	urldate = {2025-03-02},
	publisher = {arXiv},
	author = {Zhang, Yu and Wang, Mingzi and Zou, Lancheng and Liu, Wulong and Zhen, Hui-Ling and Yuan, Mingxuan and Yu, Bei},
	month = nov,
	year = {2024},
	note = {arXiv:2411.16158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	file = {PDF:/home/april/Zotero/storage/Z3DI4N8V/Zhang 等 - 2024 - MixPE Quantization and Hardware Co-design for Efficient LLM Inference.pdf:application/pdf},
}

@article{liu_acceleration_2025,
	title = {Acceleration for {Deep} {Reinforcement} {Learning} using {Parallel} and {Distributed} {Computing}: {A} {Survey}},
	volume = {57},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Acceleration for {Deep} {Reinforcement} {Learning} using {Parallel} and {Distributed} {Computing}},
	url = {https://dl.acm.org/doi/10.1145/3703453},
	doi = {10.1145/3703453},
	abstract = {Deep reinforcement learning has led to dramatic breakthroughs in the field of artificial intelligence for the past few years. As the amount of rollout experience data and the size of neural networks for deep reinforcement learning have grown continuously, handling the training process and reducing the time consumption using parallel and distributed computing is becoming an urgent and essential desire. In this article, we perform a broad and thorough investigation on training acceleration methodologies for deep reinforcement learning based on parallel and distributed computing, providing a comprehensive survey in this field with state-of-the-art methods and pointers to core references. In particular, a taxonomy of literature is provided, along with a discussion of emerging topics and open issues. This incorporates learning system architectures, simulation parallelism, computing parallelism, distributed synchronization mechanisms, and deep evolutionary reinforcement learning. Furthermore, we compare 16 current open-source libraries and platforms with criteria of facilitating rapid development. Finally, we extrapolate future directions that deserve further research.},
	language = {en},
	number = {4},
	urldate = {2025-03-02},
	journal = {ACM Computing Surveys},
	author = {Liu, Zhihong and Xu, Xin and Qiao, Peng and Li, Dongsheng},
	month = apr,
	year = {2025},
	pages = {1--35},
	file = {PDF:/home/april/Zotero/storage/YNTC99QP/Liu 等 - 2025 - Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing A Survey.pdf:application/pdf},
}

@article{mohaidat_survey_2024,
	title = {A {Survey} on {Neural} {Network} {Hardware} {Accelerators}},
	volume = {5},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2691-4581},
	url = {https://ieeexplore.ieee.org/document/10472723/},
	doi = {10.1109/TAI.2024.3377147},
	abstract = {Artiﬁcial intelligence (AI) hardware accelerator is an emerging research for several applications and domains. The hardware accelerator’s direction is to provide high computational speed with retaining low-cost and high learning performance. The main challenge is to design complex machine learning models on hardware with high performance. This article presents a thorough investigation into machine learning accelerators and associated challenges. It describes a hardware implementation of different structures such as convolutional neural network (CNN), recurrent neural network (RNN), and artiﬁcial neural network (ANN). The challenges such as speed, area, resource consumption, and throughput are discussed. It also presents a comparison between the existing hardware design. Last, the article describes the evaluation parameters for a machine learning accelerator in terms of learning and testing performance and hardware design.},
	language = {en},
	number = {8},
	urldate = {2025-03-02},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Mohaidat, Tamador and Khalil, Kasem},
	month = aug,
	year = {2024},
	pages = {3801--3822},
	file = {PDF:/home/april/Zotero/storage/4XJJWC5W/Mohaidat和Khalil - 2024 - A Survey on Neural Network Hardware Accelerators.pdf:application/pdf},
}

@misc{mo_lut_2024,
	title = {{LUT} {Tensor} {Core}: {Lookup} {Table} {Enables} {Efficient} {Low}-{Bit} {LLM} {Inference} {Acceleration}},
	shorttitle = {{LUT} {Tensor} {Core}},
	url = {http://arxiv.org/abs/2408.06003},
	doi = {10.48550/arXiv.2408.06003},
	abstract = {As large language model (LLM) inference demands ever-greater resources, there is a rapid growing trend of using low-bit weights to shrink memory usage and boost inference efficiency. However, these low-bit LLMs introduce the need for mixed-precision matrix multiplication (mpGEMM), which is a crucial yet under-explored operation that involves multiplying lower-precision weights with higher-precision activations. Unfortunately, current hardware does not natively support mpGEMM, resulting in indirect and inefficient dequantization-based implementations.},
	language = {en},
	urldate = {2025-03-02},
	publisher = {arXiv},
	author = {Mo, Zhiwen and Wang, Lei and Wei, Jianyu and Zeng, Zhichen and Cao, Shijie and Ma, Lingxiao and Jing, Naifeng and Cao, Ting and Xue, Jilong and Yang, Fan and Yang, Mao},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06003 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	file = {PDF:/home/april/Zotero/storage/9ARRYMWB/Mo et al. - 2024 - LUT Tensor Core Lookup Table Enables Efficient Low-Bit LLM Inference Acceleration.pdf:application/pdf},
}
